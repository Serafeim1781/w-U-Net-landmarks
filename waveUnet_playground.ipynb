{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "waveUnet playground",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.4 64-bit ('wave-u-net': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "780a401f402fe7e4fb4feefdf78f25be19596225b56a876dff5bbf58be5bfa34"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "JzU4ZEyZjAC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##resample.py"
      ],
      "metadata": {
        "id": "Z1b9Z8mHj1VC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "class Resample1d(nn.Module):\n",
        "    def __init__(self, channels, kernel_size, stride, transpose=False, padding=\"reflect\", trainable=False):\n",
        "        '''\n",
        "        Creates a resampling layer for time series data (using 1D convolution) - (N, C, W) input format # (N batch_size, C channels, W lentgth of downsampled time_steps)\n",
        "        :param channels: Number of features C at each time-step\n",
        "        :param kernel_size: Width of sinc-based lowpass-filter (>= 15 recommended for good filtering performance)\n",
        "        :param stride: Resampling factor (integer)\n",
        "        :param transpose: False for down-, true for upsampling\n",
        "        :param padding: Either \"reflect\" to pad or \"valid\" to not pad\n",
        "        :param trainable: Optionally activate this to train the lowpass-filter, starting from the sinc initialisation\n",
        "        '''\n",
        "        super(Resample1d, self).__init__()\n",
        "\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.transpose = transpose\n",
        "        self.channels = channels\n",
        "\n",
        "        cutoff = 0.5 / stride\n",
        "\n",
        "        assert(kernel_size > 2)\n",
        "        assert ((kernel_size - 1) % 2 == 0)\n",
        "        assert(padding == \"reflect\" or padding == \"valid\")\n",
        "\n",
        "        filter = build_sinc_filter(kernel_size, cutoff)\n",
        "\n",
        "        self.filter = torch.nn.Parameter(torch.from_numpy(np.repeat(np.reshape(filter, [1, 1, kernel_size]), channels, axis=0)), requires_grad=trainable)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pad here if not using transposed conv\n",
        "        input_size = x.shape[2]\n",
        "        # print(\"self.padding = \",self.padding )\n",
        "        if self.padding != \"valid\":\n",
        "            num_pad = (self.kernel_size-1)//2\n",
        "            out = F.pad(x, (num_pad, num_pad), mode=self.padding)\n",
        "        else:\n",
        "            out = x\n",
        "\n",
        "        # Lowpass filter (+ 0 insertion if transposed)\n",
        "        if self.transpose:\n",
        "            expected_steps = ((input_size - 1) * self.stride + 1)\n",
        "            if self.padding == \"valid\":\n",
        "                expected_steps = expected_steps - self.kernel_size + 1\n",
        "\n",
        "            out = F.conv_transpose1d(out, self.filter, stride=self.stride, padding=0, groups=self.channels)\n",
        "            diff_steps = out.shape[2] - expected_steps\n",
        "            if diff_steps > 0:\n",
        "                assert(diff_steps % 2 == 0)\n",
        "                out = out[:,:,diff_steps//2:-diff_steps//2]\n",
        "        else:\n",
        "            assert(input_size % self.stride == 1)\n",
        "            out = F.conv1d(out, self.filter, stride=self.stride, padding=0, groups=self.channels)\n",
        "        # print(\"out.size() = \", out.size())\n",
        "        return out\n",
        "\n",
        "    def get_output_size(self, input_size):\n",
        "        '''\n",
        "        Returns the output dimensionality (number of timesteps) for a given input size\n",
        "        :param input_size: Number of input time steps (Scalar, each feature is one-dimensional)\n",
        "        :return: Output size (scalar)\n",
        "        '''\n",
        "        assert(input_size > 1)\n",
        "        if self.transpose:\n",
        "            if self.padding == \"valid\":\n",
        "                return ((input_size - 1) * self.stride + 1) - self.kernel_size + 1\n",
        "            else:\n",
        "                return ((input_size - 1) * self.stride + 1)\n",
        "        else:\n",
        "            assert(input_size % self.stride == 1) # Want to take first and last sample\n",
        "            if self.padding == \"valid\":\n",
        "                return input_size - self.kernel_size + 1\n",
        "            else:\n",
        "                return input_size\n",
        "\n",
        "    def get_input_size(self, output_size):\n",
        "        '''\n",
        "        Returns the input dimensionality (number of timesteps) for a given output size\n",
        "        :param input_size: Number of input time steps (Scalar, each feature is one-dimensional)\n",
        "        :return: Output size (scalar)\n",
        "        '''\n",
        "\n",
        "        # Strided conv/decimation\n",
        "        if not self.transpose:\n",
        "            curr_size = (output_size - 1)*self.stride + 1 # o = (i-1)//s + 1 => i = (o - 1)*s + 1\n",
        "        else:\n",
        "            curr_size = output_size\n",
        "\n",
        "        # Conv\n",
        "        if self.padding == \"valid\":\n",
        "            curr_size = curr_size + self.kernel_size - 1 # o = i + p - k + 1\n",
        "\n",
        "        # Transposed\n",
        "        if self.transpose:\n",
        "            assert ((curr_size - 1) % self.stride == 0)# We need to have a value at the beginning and end\n",
        "            curr_size = ((curr_size - 1) // self.stride) + 1\n",
        "        assert(curr_size > 0)\n",
        "        return curr_size\n",
        "\n",
        "def build_sinc_filter(kernel_size, cutoff):\n",
        "    # FOLLOWING https://www.analog.com/media/en/technical-documentation/dsp-book/dsp_book_Ch16.pdf\n",
        "    # Sinc lowpass filter\n",
        "    # Build sinc kernel\n",
        "    assert(kernel_size % 2 == 1)\n",
        "    M = kernel_size - 1\n",
        "    filter = np.zeros(kernel_size, dtype=np.float32)\n",
        "    for i in range(kernel_size):\n",
        "        if i == M//2:\n",
        "            filter[i] = 2 * np.pi * cutoff\n",
        "        else:\n",
        "            filter[i] = (np.sin(2 * np.pi * cutoff * (i - M//2)) / (i - M//2)) * \\\n",
        "                    (0.42 - 0.5 * np.cos((2 * np.pi * i) / M) + 0.08 * np.cos(4 * np.pi * M))\n",
        "\n",
        "    filter = filter / np.sum(filter)\n",
        "    return filter"
      ],
      "outputs": [],
      "metadata": {
        "id": "jSLgcIHQjetq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##crop.py "
      ],
      "metadata": {
        "id": "x1J6FzcfjyWT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def centre_crop(x, target):\n",
        "    # combined = centre_crop(shortcut, upsampled)\n",
        "    '''\n",
        "    Center-crop 3-dim. input tensor along last axis so it fits the target tensor shape\n",
        "    :param x: Input tensor\n",
        "    :param target: Shape of this tensor will be used as target shape\n",
        "    :return: Cropped input tensor\n",
        "    '''\n",
        "    if x is None:\n",
        "        return None\n",
        "    if target is None:\n",
        "        return x\n",
        "\n",
        "    target_shape = target.shape\n",
        "    diff = x.shape[-1] - target_shape[-1]\n",
        "    assert (diff % 2 == 0)\n",
        "    crop = diff // 2\n",
        "\n",
        "    if crop == 0:\n",
        "        return x\n",
        "    if crop < 0:\n",
        "        raise ArithmeticError\n",
        "\n",
        "    return x[:, :, crop:-crop].contiguous()"
      ],
      "outputs": [],
      "metadata": {
        "id": "RZRdRi1cjWCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##conv.py"
      ],
      "metadata": {
        "id": "kqWtYoeWjszg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, conv_type, transpose=False):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.transpose = transpose\n",
        "        self.stride = stride\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv_type = conv_type\n",
        "\n",
        "        # How many channels should be normalised as one group if GroupNorm is activated\n",
        "        # WARNING: Number of channels has to be divisible by this number!\n",
        "        NORM_CHANNELS = 8\n",
        "\n",
        "        if self.transpose:\n",
        "            self.filter = nn.ConvTranspose1d(n_inputs, n_outputs, self.kernel_size, stride, padding=kernel_size-1)\n",
        "        else:\n",
        "            self.filter = nn.Conv1d(n_inputs, n_outputs, self.kernel_size, stride)\n",
        "\n",
        "        if conv_type == \"gn\":\n",
        "            assert(n_outputs % NORM_CHANNELS == 0)\n",
        "            self.norm = nn.GroupNorm(n_outputs // NORM_CHANNELS, n_outputs)\n",
        "        elif conv_type == \"bn\":\n",
        "            self.norm = nn.BatchNorm1d(n_outputs, momentum=0.01)\n",
        "        # Add you own types of variations here!\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the convolution\n",
        "        if self.conv_type == \"gn\" or self.conv_type == \"bn\":\n",
        "            out = F.relu(self.norm((self.filter(x))))\n",
        "        else: # Add your own variations here with elifs conditioned on \"conv_type\" parameter!\n",
        "            assert(self.conv_type == \"normal\")\n",
        "            out = F.leaky_relu(self.filter(x))\n",
        "        return out\n",
        "\n",
        "    def get_input_size(self, output_size):\n",
        "        # Strided conv/decimation\n",
        "        if not self.transpose:\n",
        "            curr_size = (output_size - 1)*self.stride + 1 # o = (i-1)//s + 1 => i = (o - 1)*s + 1\n",
        "        else:\n",
        "            curr_size = output_size\n",
        "\n",
        "        # Conv\n",
        "        curr_size = curr_size + self.kernel_size - 1 # o = i + p - k + 1\n",
        "\n",
        "        # Transposed\n",
        "        if self.transpose:\n",
        "            assert ((curr_size - 1) % self.stride == 0)# We need to have a value at the beginning and end\n",
        "            curr_size = ((curr_size - 1) // self.stride) + 1\n",
        "        assert(curr_size > 0)\n",
        "        return curr_size\n",
        "\n",
        "    def get_output_size(self, input_size):\n",
        "        # Transposed\n",
        "        if self.transpose:\n",
        "            assert(input_size > 1)\n",
        "            curr_size = (input_size - 1)*self.stride + 1 # o = (i-1)//s + 1 => i = (o - 1)*s + 1\n",
        "        else:\n",
        "            curr_size = input_size\n",
        "\n",
        "        # Conv\n",
        "        curr_size = curr_size - self.kernel_size + 1 # o = i + p - k + 1\n",
        "        assert (curr_size > 0)\n",
        "\n",
        "        # Strided conv/decimation\n",
        "        if not self.transpose:\n",
        "            assert ((curr_size - 1) % self.stride == 0)  # We need to have a value at the beginning and end\n",
        "            curr_size = ((curr_size - 1) // self.stride) + 1\n",
        "\n",
        "        return curr_size"
      ],
      "outputs": [],
      "metadata": {
        "id": "a_i7kl6_jSYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##UTILS.py"
      ],
      "metadata": {
        "id": "4xMpyXxUjn41"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "import os\n",
        "\n",
        "def save_model(model, optimizer, state, path):\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model = model.module  # save state dict of wrapped module\n",
        "    if len(os.path.dirname(path)) > 0 and not os.path.exists(os.path.dirname(path)):\n",
        "        os.makedirs(os.path.dirname(path))\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'state': state,  # state of training loop (was 'step')\n",
        "    }, path)\n",
        "\n",
        "\n",
        "def load_model(model, optimizer, path, cuda):\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model = model.module  # load state dict of wrapped module\n",
        "    if cuda:\n",
        "        checkpoint = torch.load(path)\n",
        "    else:\n",
        "        checkpoint = torch.load(path, map_location='cpu')\n",
        "    try:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    except:\n",
        "        # work-around for loading checkpoints where DataParallel was saved instead of inner module\n",
        "        from collections import OrderedDict\n",
        "        model_state_dict_fixed = OrderedDict()\n",
        "        prefix = 'module.'\n",
        "        for k, v in checkpoint['model_state_dict'].items():\n",
        "            if k.startswith(prefix):\n",
        "                k = k[len(prefix):]\n",
        "            model_state_dict_fixed[k] = v\n",
        "        model.load_state_dict(model_state_dict_fixed)\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if 'state' in checkpoint:\n",
        "        state = checkpoint['state']\n",
        "    else:\n",
        "        # older checkpoints only store step, rest of state won't be there\n",
        "        state = {'step': checkpoint['step']}\n",
        "    return state\n",
        "\n",
        "\n",
        "def compute_loss(model, inputs, targets, criterion, compute_grad=False):\n",
        "    '''\n",
        "    Computes gradients of model with given inputs and targets and loss function.\n",
        "    Optionally backpropagates to compute gradients for weights.\n",
        "    Procedure depends on whether we have one model for each source or not\n",
        "    :param model: Model to train with\n",
        "    :param inputs: Input mixture\n",
        "    :param targets: Target sources\n",
        "    :param criterion: Loss function to use (L1, L2, ..)\n",
        "    :param compute_grad: Whether to compute gradients\n",
        "    :return: Model outputs, Average loss over batch\n",
        "    '''\n",
        "    all_outputs = {}\n",
        "\n",
        "    if model.separate:\n",
        "        avg_loss = 0.0\n",
        "        num_sources = 0\n",
        "        for inst in model.speakers:\n",
        "            output = model(inputs, inst)\n",
        "            loss = criterion(output[inst], targets[inst])\n",
        "\n",
        "            if compute_grad:\n",
        "                loss.backward()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "            num_sources += 1\n",
        "\n",
        "            all_outputs[inst] = output[inst].detach().clone()\n",
        "\n",
        "        avg_loss /= float(num_sources)\n",
        "    else:\n",
        "        loss = 0\n",
        "        all_outputs = model(inputs)\n",
        "        for inst in all_outputs.keys():\n",
        "            loss += criterion(all_outputs[inst], targets[inst])\n",
        "\n",
        "        if compute_grad:\n",
        "            loss.backward()\n",
        "\n",
        "        avg_loss = loss.item() / float(len(all_outputs))\n",
        "\n",
        "    return all_outputs, avg_loss\n",
        "\n",
        "\n",
        "class DataParallel(torch.nn.DataParallel):\n",
        "    def __init__(self, module, device_ids=None, output_device=None, dim=0):\n",
        "        super(DataParallel, self).__init__(module, device_ids, output_device, dim)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        try:\n",
        "            return super().__getattr__(name)\n",
        "        except AttributeError:\n",
        "            return getattr(self.module, name)"
      ],
      "outputs": [],
      "metadata": {
        "id": "kBDv7Xc2jjPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#waveunet.py"
      ],
      "metadata": {
        "id": "EkeFqzjrj5Dc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "class UpsamplingBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res):\n",
        "        super(UpsamplingBlock, self).__init__()\n",
        "        assert(stride > 1)\n",
        "\n",
        "        # CONV 1 for UPSAMPLING\n",
        "        if res == \"fixed\":\n",
        "            self.upconv = Resample1d(n_inputs, 15, stride, transpose=True)\n",
        "        else:\n",
        "            self.upconv = ConvLayer(n_inputs, n_inputs, kernel_size, stride, conv_type, transpose=True)\n",
        "\n",
        "        self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_outputs, kernel_size, 1, conv_type)] +\n",
        "                                                [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
        "\n",
        "        # CONVS to combine high- with low-level information (from shortcut)\n",
        "        self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_outputs + n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
        "                                                 [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
        "\n",
        "    def forward(self, x, shortcut):\n",
        "        # UPSAMPLE HIGH-LEVEL FEATURES\n",
        "        upsampled = self.upconv(x)\n",
        "\n",
        "        for conv in self.pre_shortcut_convs:\n",
        "            upsampled = conv(upsampled)\n",
        "\n",
        "        # Prepare shortcut connection\n",
        "        combined = centre_crop(shortcut, upsampled) # shortcut has to have the same size as the shortcut\n",
        "\n",
        "        # Combine high- and low-level features\n",
        "        for conv in self.post_shortcut_convs:\n",
        "            combined = conv(torch.cat([combined, centre_crop(upsampled, combined)], dim=1)) #[0,1,2]\n",
        "        return combined\n",
        "\n",
        "    def get_output_size(self, input_size):\n",
        "        curr_size = self.upconv.get_output_size(input_size)\n",
        "\n",
        "        # Upsampling convs\n",
        "        for conv in self.pre_shortcut_convs:\n",
        "            curr_size = conv.get_output_size(curr_size)\n",
        "\n",
        "        # Combine convolutions\n",
        "        for conv in self.post_shortcut_convs:\n",
        "            curr_size = conv.get_output_size(curr_size)\n",
        "\n",
        "        return curr_size\n",
        "\n",
        "class DownsamplingBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res):\n",
        "        super(DownsamplingBlock, self).__init__()\n",
        "        assert(stride > 1)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # CONV 1\n",
        "        self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_shortcut, kernel_size, 1, conv_type)] +\n",
        "                                                [ConvLayer(n_shortcut, n_shortcut, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
        "\n",
        "        self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
        "                                                 [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in\n",
        "                                                  range(depth - 1)])\n",
        "\n",
        "        # CONV 2 with decimation\n",
        "        if res == \"fixed\":\n",
        "            self.downconv = Resample1d(n_outputs, 15, stride) # Resampling with fixed-size sinc lowpass filter\n",
        "        else:\n",
        "            self.downconv = ConvLayer(n_outputs, n_outputs, kernel_size, stride, conv_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # PREPARING SHORTCUT FEATURES\n",
        "        shortcut = x\n",
        "        for conv in self.pre_shortcut_convs:\n",
        "            shortcut = conv(shortcut)\n",
        "\n",
        "        # PREPARING FOR DOWNSAMPLING\n",
        "        out = shortcut\n",
        "        for conv in self.post_shortcut_convs:\n",
        "            out = conv(out)\n",
        "\n",
        "        # DOWNSAMPLING\n",
        "        out = self.downconv(out)\n",
        "\n",
        "        return out, shortcut\n",
        "\n",
        "    def get_input_size(self, output_size):\n",
        "        curr_size = self.downconv.get_input_size(output_size)\n",
        "\n",
        "        for conv in reversed(self.post_shortcut_convs):\n",
        "            curr_size = conv.get_input_size(curr_size)\n",
        "\n",
        "        for conv in reversed(self.pre_shortcut_convs):\n",
        "            curr_size = conv.get_input_size(curr_size)\n",
        "        return curr_size\n",
        "\n",
        "class Waveunet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, num_outputs, speakers, kernel_size, target_output_size, conv_type, res, separate=False, depth=1, strides=2):\n",
        "        super(Waveunet, self).__init__()\n",
        "\n",
        "        self.num_levels = len(num_channels)\n",
        "        self.strides = strides\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.depth = depth\n",
        "        self.speakers = speakers\n",
        "        self.separate = separate\n",
        "\n",
        "        # Only odd filter kernels allowed\n",
        "        assert(kernel_size % 2 == 1)\n",
        "\n",
        "        self.waveunets = nn.ModuleDict()\n",
        "\n",
        "        model_list = speakers if separate else [\"ALL\"]\n",
        "        # Create a model for each source if we separate sources separately, otherwise only one (model_list=[\"ALL\"])\n",
        "        for speaker in model_list:\n",
        "            module = nn.Module()\n",
        "\n",
        "            module.downsampling_blocks = nn.ModuleList()\n",
        "            module.upsampling_blocks = nn.ModuleList()\n",
        "\n",
        "            for i in range(self.num_levels - 1):\n",
        "                in_ch = num_inputs if i == 0 else num_channels[i]\n",
        "\n",
        "                module.downsampling_blocks.append(\n",
        "                    DownsamplingBlock(in_ch, num_channels[i], num_channels[i+1], kernel_size, strides, depth, conv_type, res))\n",
        "\n",
        "            for i in range(0, self.num_levels - 1):\n",
        "                module.upsampling_blocks.append(\n",
        "                    UpsamplingBlock(num_channels[-1-i], num_channels[-2-i], num_channels[-2-i], kernel_size, strides, depth, conv_type, res))\n",
        "\n",
        "            module.bottlenecks = nn.ModuleList(\n",
        "                [ConvLayer(num_channels[-1], num_channels[-1], kernel_size, 1, conv_type) for _ in range(depth)])\n",
        "\n",
        "            # Output conv\n",
        "            outputs = num_outputs if separate else num_outputs * len(speakers)\n",
        "            module.output_conv = nn.Conv1d(num_channels[0], outputs, 1)\n",
        "\n",
        "            self.waveunets[speaker] = module\n",
        "\n",
        "        self.set_output_size(target_output_size)\n",
        "\n",
        "    def set_output_size(self, target_output_size):\n",
        "        self.target_output_size = target_output_size\n",
        "\n",
        "        self.input_size, self.output_size = self.check_padding(target_output_size)\n",
        "        print(\"Using valid convolutions with \" + str(self.input_size) + \" inputs and \" + str(self.output_size) + \" outputs\")\n",
        "\n",
        "        assert((self.input_size - self.output_size) % 2 == 0)\n",
        "        self.shapes = {\"output_start_frame\" : (self.input_size - self.output_size) // 2,\n",
        "                       \"output_end_frame\" : (self.input_size - self.output_size) // 2 + self.output_size,\n",
        "                       \"output_frames\" : self.output_size,\n",
        "                       \"input_frames\" : self.input_size}\n",
        "\n",
        "    def check_padding(self, target_output_size):\n",
        "        # Ensure number of outputs covers a whole number of cycles so each output in the cycle is weighted equally during training\n",
        "        bottleneck = 1\n",
        "\n",
        "        while True:\n",
        "            out = self.check_padding_for_bottleneck(bottleneck, target_output_size)\n",
        "            if out is not False:\n",
        "                return out\n",
        "            bottleneck += 1\n",
        "\n",
        "    def check_padding_for_bottleneck(self, bottleneck, target_output_size):\n",
        "        module = self.waveunets[[k for k in self.waveunets.keys()][0]]\n",
        "        try:\n",
        "            curr_size = bottleneck\n",
        "            for idx, block in enumerate(module.upsampling_blocks):\n",
        "                curr_size = block.get_output_size(curr_size)\n",
        "            output_size = curr_size\n",
        "\n",
        "            # Bottleneck-Conv\n",
        "            curr_size = bottleneck\n",
        "            for block in reversed(module.bottlenecks):\n",
        "                curr_size = block.get_input_size(curr_size)\n",
        "            for idx, block in enumerate(reversed(module.downsampling_blocks)):\n",
        "                curr_size = block.get_input_size(curr_size)\n",
        "\n",
        "            assert(output_size >= target_output_size)\n",
        "            return curr_size, output_size\n",
        "        except AssertionError as e:\n",
        "            return False\n",
        "\n",
        "    def forward_module(self, x, module):\n",
        "        '''\n",
        "        A forward pass through a single Wave-U-Net (multiple Wave-U-Nets might be used, one for each source)\n",
        "        :param x: Input mix\n",
        "        :param module: Network module to be used for prediction\n",
        "        :return: Source estimates\n",
        "        '''\n",
        "        shortcuts = []\n",
        "        out = x\n",
        "\n",
        "        # DOWNSAMPLING BLOCKS\n",
        "        for block in module.downsampling_blocks:\n",
        "            out, short = block(out)\n",
        "            shortcuts.append(short)\n",
        "        print(\"out.size() pre bottlenet\",out.size())\n",
        "        # BOTTLENECK CONVOLUTION\n",
        "        for conv in module.bottlenecks:\n",
        "            out = conv(out)\n",
        "        print(\"out.size() after = \", out.size())\n",
        "        # UPSAMPLING BLOCKS\n",
        "        for idx, block in enumerate(module.upsampling_blocks):\n",
        "            out = block(out, shortcuts[-1 - idx])\n",
        "\n",
        "        # OUTPUT CONV\n",
        "        out = module.output_conv(out)\n",
        "        if not self.training:  # At test time clip predictions to valid amplitude range\n",
        "            out = out.clamp(min=-1.0, max=1.0)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, inst=None):\n",
        "        curr_input_size = x.shape[-1]\n",
        "        # print('self.waveunets ',self.waveunets.keys())\n",
        "        # print('self.input_size ',self.input_size)\n",
        "        assert(curr_input_size == self.input_size) # User promises to feed the proper input himself, to get the pre-calculated (NOT the originally desired) output size\n",
        "\n",
        "        if self.separate:\n",
        "            return {inst : self.forward_module(x, self.waveunets[inst])}\n",
        "        else:\n",
        "            assert(len(self.waveunets) == 1)\n",
        "            out = self.forward_module(x, self.waveunets[\"ALL\"])\n",
        "\n",
        "            out_dict = {}\n",
        "            for idx, inst in enumerate(self.speakers):\n",
        "                out_dict[inst] = out[:, idx * self.num_outputs:(idx + 1) * self.num_outputs]\n",
        "            return out_dict"
      ],
      "outputs": [],
      "metadata": {
        "id": "KPx7EHR6j6ho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# model = Waveunet(num_inputs =2, num_channels = [1], num_outputs = 2,speakers =[\"bass\", \"drums\", \"other\", \"vocals\"],kernel_size= 5, target_output_size = 2.0,conv_type = \"gn\", res = \"fixed\"  )\n",
        "# class Waveunet(nn.Module):\n",
        "#     def __init__(self, num_inputs, num_channels, num_outputs, speakers, kernel_size, target_output_size, conv_type, res, separate=False, depth=1, strides=2):\n",
        "#         super(Waveunet, self).__init__()"
      ],
      "outputs": [],
      "metadata": {
        "id": "mqTQcCQZj-x5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "len([1])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8wG9wX-nAnO",
        "outputId": "df364d75-8154-495e-d17a-1d25e438e8b0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# import argparse\n",
        "# parser = argparse.ArgumentParser(description='Optional app description')\n",
        "\n",
        "# parser.add_argument('--speakers', type=str, nargs='+', default=[\"bass\", \"drums\", \"other\", \"vocals\"],\n",
        "#                     help=\"List of speakers to separate (default: \\\"bass drums other vocals\\\")\")\n",
        "# parser.add_argument('--cuda', action='store_true',\n",
        "#                     help='Use CUDA (default: False)')\n",
        "# parser.add_argument('--num_workers', type=int, default=1,\n",
        "#                     help='Number of data loader worker threads (default: 1)')\n",
        "# parser.add_argument('--features', type=int, default=32,\n",
        "#                     help='Number of feature channels per layer')\n",
        "# parser.add_argument('--log_dir', type=str, default='logs/waveunet',\n",
        "#                     help='Folder to write logs into')\n",
        "# parser.add_argument('--dataset_dir', type=str, default=\"/mnt/windaten/Datasets/MUSDB18HQ\",\n",
        "#                     help='Dataset path')\n",
        "# parser.add_argument('--hdf_dir', type=str, default=\"hdf\",\n",
        "#                     help='Dataset path')\n",
        "# parser.add_argument('--checkpoint_dir', type=str, default='checkpoints/waveunet',\n",
        "#                     help='Folder to write checkpoints into')\n",
        "# parser.add_argument('--load_model', type=str, default=None,\n",
        "#                     help='Reload a previously trained model (whole task model)')\n",
        "# parser.add_argument('--lr', type=float, default=1e-3,\n",
        "#                     help='Initial learning rate in LR cycle (default: 1e-3)')\n",
        "# parser.add_argument('--min_lr', type=float, default=5e-5,\n",
        "#                     help='Minimum learning rate in LR cycle (default: 5e-5)')\n",
        "# parser.add_argument('--cycles', type=int, default=2,\n",
        "#                     help='Number of LR cycles per epoch')\n",
        "# parser.add_argument('--batch_size', type=int, default=4,\n",
        "#                     help=\"Batch size\")\n",
        "# parser.add_argument('--levels', type=int, default=6,\n",
        "#                     help=\"Number of DS/US blocks\")\n",
        "# parser.add_argument('--depth', type=int, default=1,\n",
        "#                     help=\"Number of convs per block\")\n",
        "# parser.add_argument('--sr', type=int, default=44100,\n",
        "#                     help=\"Sampling rate\")\n",
        "# parser.add_argument('--channels', type=int, default=2,\n",
        "#                     help=\"Number of input audio channels\")\n",
        "# parser.add_argument('--kernel_size', type=int, default=5,\n",
        "#                     help=\"Filter width of kernels. Has to be an odd number\")\n",
        "# parser.add_argument('--output_size', type=float, default=2.0,\n",
        "#                     help=\"Output duration\")\n",
        "# parser.add_argument('--strides', type=int, default=4,\n",
        "#                     help=\"Strides in Waveunet\")\n",
        "# parser.add_argument('--patience', type=int, default=20,\n",
        "#                     help=\"Patience for early stopping on validation set\")\n",
        "# parser.add_argument('--example_freq', type=int, default=200,\n",
        "#                     help=\"Write an audio summary into Tensorboard logs every X training iterations\")\n",
        "# parser.add_argument('--loss', type=str, default=\"L1\",\n",
        "#                     help=\"L1 or L2\")\n",
        "# parser.add_argument('--conv_type', type=str, default=\"gn\",\n",
        "#                     help=\"Type of convolution (normal, BN-normalised, GN-normalised): normal/bn/gn\")\n",
        "# parser.add_argument('--res', type=str, default=\"fixed\",\n",
        "#                     help=\"Resampling strategy: fixed sinc-based lowpass filtering or learned conv layer: fixed/learned\")\n",
        "# parser.add_argument('--separate', type=int, default=1,\n",
        "#                     help=\"Train separate model for each source (1) or only one (0)\")\n",
        "# parser.add_argument('--feature_growth', type=str, default=\"double\",\n",
        "#                     help=\"How the features in each layer should grow, either (add) the initial number of features each time, or multiply by 2 (double)\")\n",
        "\n",
        "# args = parser.parse_args()\n",
        "feature_growth = \"double\"\n",
        "levels = 6\n",
        "num_features = [32*i for i in range(1, levels +1)] if feature_growth == \"add\" else \\\n",
        "                [32*2**i for i in range(0, 6)]\n",
        "# num_features = [args.features*i for i in range(1, args.levels+1)] if args.feature_growth == \"add\" else \\\n",
        "#                 [args.features*2**i for i in range(0, args.levels)]\n",
        "output_size = 2.0\n",
        "sr  = 16e3#44100\n",
        "target_outputs = int(output_size * sr)\n",
        "num_ch = 1\n",
        "# target_outputs = int(args.output_size * args.sr)\n",
        "\n",
        "# model = Waveunet(num_inputs =2, num_channels =num_features , num_outputs = 2,speakers =[\"bass\", \"drums\", \"other\", \"vocals\"],kernel_size= 5, target_output_size = 2.0,depth=1,strides =4,conv_type = \"gn\",res = \"fixed\",separate = 1)\n",
        "# class Waveunet(nn.Module):\n",
        "# model = Waveunet(num_inputs =2, num_channels =num_features , num_outputs = 2,speakers =[\"ALL\"],kernel_size= 5, target_output_size = 2.0,depth=1,strides =4,conv_type = \"gn\",res = \"fixed\",separate = False)\n",
        "# class Waveunet(nn.Module):\n",
        "# model = Waveunet(args.channels, num_features, args.channels, args.speakers, kernel_size=args.kernel_size,\n",
        "#                   target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
        "#                   conv_type=args.conv_type, res=args.res, separate=args.separate)\n",
        "\n",
        "# model = Waveunet(args.channels, num_features, args.channels, args.speakers, kernel_size=args.kernel_size,\n",
        "#                   target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
        "#                   conv_type=args.conv_type, res=args.res, separate=args.separate)"
      ],
      "outputs": [],
      "metadata": {
        "id": "lHzJlZydl3a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "model = Waveunet(num_inputs =num_ch, num_channels =num_features , num_outputs = num_ch,speakers =[\"1\", \"2\"],kernel_size= 3, target_output_size = target_outputs,depth=1,strides =4,conv_type = \"gn\",res = \"fixed\",separate = False)\n",
        "\n",
        "num_features"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using valid convolutions with 37205 inputs and 32429 outputs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 64, 128, 256, 512, 1024]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SDN8HycSzec",
        "outputId": "1a90e887-c2dd-4415-e838-76048bd6fb10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "N_batch=5\n",
        "num_ch = 1\n",
        "L_timeseries = 37205                \n",
        "input = torch.rand([N_batch, num_ch,L_timeseries])\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "jEUCrK9YPNtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "with torch.no_grad():\n",
        "  out = model(input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out.size() pre bottlenet torch.Size([5, 1024, 36])\n",
            "out.size() after =  torch.Size([5, 1024, 34])\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91HJNkZ8XmV2",
        "outputId": "2d4b0420-e439-4344-f1be-960287dd2307"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "out['1'].size()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 1, 32429])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gsbZQqzpR3E",
        "outputId": "53dfe575-0f82-455a-f928-9434237a54a7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Waveunet(\n",
        "#   (waveunets): ModuleDict(\n",
        "#     (ALL): Module(\n",
        "#       (downsampling_blocks): ModuleList(\n",
        "#         (0): DownsamplingBlock(\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (downconv): Resample1d()\n",
        "#         )\n",
        "#         (1): DownsamplingBlock(\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (downconv): Resample1d()\n",
        "#         )\n",
        "#         (2): DownsamplingBlock(\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (downconv): Resample1d()\n",
        "#         )\n",
        "#         (3): DownsamplingBlock(\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (downconv): Resample1d()\n",
        "#         )\n",
        "#         (4): DownsamplingBlock(\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (downconv): Resample1d()\n",
        "#         )\n",
        "#       )\n",
        "#       (upsampling_blocks): ModuleList(\n",
        "#         (0): UpsamplingBlock(\n",
        "#           (upconv): Resample1d()\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#         )\n",
        "#         (1): UpsamplingBlock(\n",
        "#           (upconv): Resample1d()\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#         )\n",
        "#         (2): UpsamplingBlock(\n",
        "#           (upconv): Resample1d()\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#         )\n",
        "#         (3): UpsamplingBlock(\n",
        "#           (upconv): Resample1d()\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#         )\n",
        "#         (4): UpsamplingBlock(\n",
        "#           (upconv): Resample1d()\n",
        "#           (pre_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#           (post_shortcut_convs): ModuleList(\n",
        "#             (0): ConvLayer(\n",
        "#               (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
        "#               (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
        "#             )\n",
        "#           )\n",
        "#         )\n",
        "#       )\n",
        "#       (bottlenecks): ModuleList(\n",
        "#         (0): ConvLayer(\n",
        "#           (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
        "#           (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
        "#         )\n",
        "#       )\n",
        "#       (output_conv): Conv1d(32, 8, kernel_size=(1,), stride=(1,))\n",
        "#     )\n",
        "#   )\n",
        "# )\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "wlhbZ8fSQHtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "D = 5\n",
        "I = 1\n",
        "N =1000\n",
        "V = 12\n",
        "\n",
        "a = torch.rand([D,N])\n",
        "b = torch.rand([V,N])\n",
        "ab = torch.cat([a,b]).unsqueeze(dim = 1)\n",
        "print(ab.size())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([17, 1, 1000])\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B7Go7YJ2CJu",
        "outputId": "d64daf49-4cc6-41b0-978c-c0d8f708178c"
      }
    }
  ]
}